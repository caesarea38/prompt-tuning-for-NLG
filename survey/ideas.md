# Ideas around the human based evaluation survey

This file collects our ideas in how to structure and conduct a human based evaluation of our project results.

## What to ask
* Is the given text snippet created by a human person or by a machine
* Does the given text snippet sound natural
* Does the given text snippet contain (grammatical) errors
* ...

## How to sample the data
* Take outputs from all the different models / experiments as text snippet
* Take labels as text snippet
* Take human created text snippets
* Take human created text snippets where the human is not native speaker
  * maybe results in small errors in the text snippet
  * maybe results in unnatural sentence construction
  * can serve as blurring out whether text snippets are human or machine generated
* ...

## Open questions
* how many different sets
* how many items per set
* ...