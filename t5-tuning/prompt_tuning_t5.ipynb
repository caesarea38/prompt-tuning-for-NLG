{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Installing the required packages"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OIoYkDMph0qI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uemA2s_7h0qO"
   },
   "outputs": [],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing required libraries"
   ],
   "metadata": {
    "collapsed": false,
    "id": "qtPNBfoDh0qQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_scheduler\n",
    "from transformers.optimization import Adafactor, AdamW\n",
    "from IPython.display import HTML, display"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2gK6uqcrh0qQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Use this when working on Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9e6cPibh0qR",
    "outputId": "8086dd73-3028-41f9-e7ed-c0d8dc0d34a0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Class for Prompt Tuning"
   ],
   "metadata": {
    "id": "BNixl_L6qRAJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class T5PromptTuning(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "          super().__init__(config)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_path: str, soft_prompt_path: str = None, number_tokens: int = None,\n",
    "        initialize_from_vocab: bool = True, random_range: float = 0.5, **kwargs):\n",
    "      \n",
    "        model = super().from_pretrained(model_name_or_path, **kwargs)\n",
    "  \n",
    "        #  freeze the transformers model\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # if a saved soft prompt is loaded, use its embeddings\n",
    "        if soft_prompt_path is not None:\n",
    "            model.set_soft_prompt_embeds(soft_prompt_path)\n",
    "\n",
    "        # else create a new soft prompt\n",
    "        elif number_tokens is not None:\n",
    "            print(\"Initializing soft prompt...\")\n",
    "            model.initialize_soft_prompt(\n",
    "                number_tokens=number_tokens, initialize_from_vocab=initialize_from_vocab, random_range=random_range)\n",
    "        return model\n",
    "\n",
    "    def set_soft_prompt_embeds(self, soft_prompt_path: str):\n",
    "        self.soft_prompt = torch.load(\n",
    "            soft_prompt_path, map_location=torch.device(\"cpu\")\n",
    "        )\n",
    "        self.number_tokens = self.soft_prompt.num_embeddings\n",
    "        print(f\"Set soft prompt. (number_tokens: {self.number_tokens})\")\n",
    "\n",
    "    def initialize_soft_prompt(self, number_tokens: int = 20, initialize_from_vocab: bool = True, random_range: float = 0.5):\n",
    "        self.number_tokens = number_tokens\n",
    "        init_prompt_value = None\n",
    "        if initialize_from_vocab:\n",
    "            init_prompt_value = self.shared.weight[:number_tokens].clone().detach()\n",
    "        else:\n",
    "            init_prompt_value = torch.FloatTensor(number_tokens, self.config.d_model).uniform_(-random_range, random_range)\n",
    "        self.soft_prompt = torch.nn.Embedding(number_tokens, self.config.d_model)\n",
    "\n",
    "        # Initialize weight\n",
    "        self.soft_prompt.weight = torch.nn.parameter.Parameter(init_prompt_value)\n",
    "        #print(self.soft_prompt.weight.shape)\n",
    "\n",
    "    # this method appends the learned prompt embeddings to the input ids of the input before the\n",
    "    # the forward pass is calculated\n",
    "    def append_learned_embedding_to_input(self, input_ids):\n",
    "        inputs_embeds = self.shared(input_ids)\n",
    "        #print(inputs_embeds.shape)\n",
    "        if len(list(inputs_embeds.shape)) == 2: inputs_embeds = inputs_embeds.unsqueeze(0)\n",
    "\n",
    "        # the shape of the tensor that will be returned will be: [batch_size, max_sequence_length, number_embeddings] -> [8, 600, 512]\n",
    "        learned_embeds = self.soft_prompt.weight.repeat(inputs_embeds.size(0), 1, 1)\n",
    "\n",
    "        print('shape learned embeds: ' + str(learned_embeds.shape))\n",
    "\n",
    "        inputs_embeds = torch.cat([learned_embeds, inputs_embeds], dim=1)\n",
    "\n",
    "        #print('shape inputs embeds: ' + str(inputs_embeds.shape))\n",
    "        return inputs_embeds\n",
    "\n",
    "    # to make sure that padding token ids of the labels are not taken into account by the loss function\n",
    "    # this method extends the labels tensor by elements that are ignored by the CrossEntropyLoss function\n",
    "    # this can be done using the ignore_index value -100\n",
    "    def extend_labels(self, labels, ignore_index=-100):\n",
    "        if len(list(labels.shape)) == 1: labels = labels.unsqueeze(0)\n",
    "        number_of_batches = labels.shape[0]\n",
    "\n",
    "        #print('number batches: ' + str(n_batches))\n",
    "\n",
    "        # return a new tensor of shape [number_of_batches, number_tokens+labels] that is filled with the ignore_index value (-100)\n",
    "        return torch.cat([torch.full((number_of_batches, self.number_tokens), ignore_index).to(self.device), labels], dim=1)\n",
    "\n",
    "    def extend_attention_mask(self, attention_mask):\n",
    "        # prepend a new dimension (1) to the shape of attention_mask in case it is one dimensional\n",
    "        if len(list(attention_mask.shape)) == 1: attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "        # get the number of batches\n",
    "        number_of_batches = attention_mask.shape[0]\n",
    "\n",
    "        # return a new tensor of shape [number_of_batches, number_tokens+attention_mask] that is filled with the ones\n",
    "        return torch.cat([torch.full((number_of_batches, self.number_tokens), 1).to(self.device), attention_mask], dim=1)\n",
    "\n",
    "    def save_soft_prompt(self, filename: str = \"soft_prompt.model\"):\n",
    "        torch.save(self.soft_prompt, 't5-tuning/soft_prompts/'+ filename)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None,       past_key_values=None, attention_mask=None, token_type_ids=None,\n",
    "        position_ids=None,          head_mask=None,       inputs_embeds=None,  encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,labels=None,          use_cache=None,      output_attentions=None,\n",
    "        output_hidden_states=None,  return_dict=None):\n",
    "      \n",
    "        if input_ids is not None:\n",
    "            print('test1')\n",
    "            inputs_embeds = self.append_learned_embedding_to_input(input_ids).to(self.device)\n",
    "\n",
    "        if labels is not None:\n",
    "            print('test2')\n",
    "            labels = self.extend_labels(labels).to(self.device)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            print('test')\n",
    "            attention_mask = self.extend_attention_mask(attention_mask).to(self.device)\n",
    "\n",
    "        print(inputs_embeds)\n",
    "        return super().forward(\n",
    "            #input_ids=input_ids,\n",
    "            #attention_mask=attention_mask,\n",
    "            #inputs_embeds=inputs_embeds,\n",
    "            #labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "# This class acts as wrapper class of the T5ForConditionalGeneration class to create and initialize a soft prompt.\n",
    "# Additionally, it preprocesses the input ids and labels if the forward() method of the model is called and then calls super.forward()\n",
    "# to actually perform the forward propagation\n",
    "#class T5PromptTuningLM(T5PromptTuning, T5ForConditionalGeneration):\n",
    "#    def __init__(self, config):\n",
    "#        super().__init__(config)"
   ],
   "metadata": {
    "id": "YU0rg9NRxbkm"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the Pre-trained model T5 and the tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "id": "MxsDKQlsh0qR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_utils.py\", line 69, in attach_to_debugger\n",
      "    debugger.prepare_to_run(enable_tracing_from_start=False)\n",
      "TypeError: prepare_to_run() got an unexpected keyword argument 'enable_tracing_from_start'\n",
      "Failed to connect to target debugger.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n",
      "Initializing soft prompt...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'decoder_input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/sk/1xydrhpj73l4lx9zhtg12mmc0000gn/T/ipykernel_9742/3760849365.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[0mmodel_t5_small\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m outputs = model_t5_small.generate(input_ids=inputs['input_ids'],\n\u001B[0m\u001B[1;32m     70\u001B[0m                                   attention_mask=inputs['attention_mask'])\n\u001B[1;32m     71\u001B[0m \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/prompt-tuning-for-NLG/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/prompt-tuning-for-NLG/lib/python3.8/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m    988\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    989\u001B[0m             \u001B[0;31m# greedy search\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 990\u001B[0;31m             return self.greedy_search(\n\u001B[0m\u001B[1;32m    991\u001B[0m                 \u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    992\u001B[0m                 \u001B[0mlogits_processor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlogits_processor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/prompt-tuning-for-NLG/lib/python3.8/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgreedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1290\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1291\u001B[0m             \u001B[0;31m# forward pass to get next token\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1292\u001B[0;31m             outputs = self(\n\u001B[0m\u001B[1;32m   1293\u001B[0m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1294\u001B[0m                 \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/prompt-tuning-for-NLG/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'decoder_input_ids'"
     ]
    }
   ],
   "source": [
    "import torch as nn\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Prompt-tuning\n",
    "# number of prompt tokens\n",
    "number_prompt_tokens = 20\n",
    "\n",
    "# If set to true, the soft prompt will be initialized from the models vocabulary\n",
    "# Otherwise, it will be randomly (uniformly in a range) initialized.\n",
    "random_range = 0.5\n",
    "init_from_vocab = False\n",
    "\n",
    "\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Instantiate one T5 small model that should be trained on all the 3 datasets\n",
    "model_t5_small = T5PromptTuning.from_pretrained('t5-small', number_tokens=number_prompt_tokens, initialize_from_vocab=init_from_vocab)\n",
    "\n",
    "#moving the models to device(GPU/CPU)\n",
    "model_t5_small.to(dev)\n",
    "\n",
    "class Config:\n",
    "    # Same default parameters as run_clm_no_trainer.py in tranformers\n",
    "    # https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "    num_train_epochs = 3\n",
    "    weight_decay = 0.01\n",
    "    learning_rate = 0.01\n",
    "    lr_scheduler_type = \"linear\"\n",
    "    num_warmup_steps = 0\n",
    "    max_train_steps = num_train_epochs\n",
    "    \n",
    "    # Prompt-tuning\n",
    "    # number of prompt tokens\n",
    "    n_prompt_tokens = 20\n",
    "    # If True, soft prompt will be initialized from vocab \n",
    "    # Otherwise, you can set `random_range` to initialize by randomization.\n",
    "    init_from_vocab = False\n",
    "    # random_range = 0.5\n",
    "args = Config()\n",
    "\n",
    "# Prepare dataset\n",
    "inputs = tokenizer_t5_small(\"Hello, my dog is cute\", return_tensors=\"pt\").to(dev)\n",
    "\n",
    "# Only update soft prompt'weights for prompt-tuning. ie, all weights in LM are set as `require_grad=False`. \n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model_t5_small.named_parameters() if n == \"soft_prompt.weight\"],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "model_t5_small.train()\n",
    "outputs = model_t5_small.generate(input_ids=inputs['input_ids'],\n",
    "                                  attention_mask=inputs['attention_mask'])\n",
    "loss = outputs.loss\n",
    "print(f\"loss: {loss}\")\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KyZrfrkoh0qS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "outputId": "867403d2-4e99-42c9-86c5-0d90b9dbb6a2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YA92MUkIh0qU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "load_no_duplicate_sets = True\n",
    "\n",
    "# Load the datasets for the Web NLG 2020 challenge\n",
    "train_data_web_nlg = pd.read_csv('drive/MyDrive/MIwDL/data/web_nlg/train/webNLG2020_train.csv' if not load_no_duplicate_sets else 'drive/MyDrive/MIwDL/data/web_nlg/train/webNLG2020_train_no_duplicate_inputs.csv')\n",
    "test_data_web_nlg = pd.read_csv('drive/MyDrive/MIwDL/data/web_nlg/test/webNLG2020_test.csv' if not load_no_duplicate_sets else 'drive/MyDrive/MIwDL/data/web_nlg/test/webNLG2020_test_no_duplicate_inputs.csv')\n",
    "\n",
    "# Load the datasets for the Meaning Representation E2E challenge\n",
    "train_data_e2e = pd.read_csv('drive/MyDrive/MIwDL/data/e2e/train/trainset.csv' if not load_no_duplicate_sets else 'drive/MyDrive/MIwDL/data/e2e/train/trainset_no_duplicate_inputs.csv')\n",
    "test_data_e2e = pd.read_csv('drive/MyDrive/MIwDL/data/e2e/test/testset_w_refs.csv' if not load_no_duplicate_sets else 'drive/MyDrive/MIwDL/data/e2e/test/testset_no_duplicate_inputs.csv')\n",
    "\n",
    "# Load the datasets for the Abstract Meaning Representation AMR challenge\n",
    "train_data_amr = pd.read_csv('drive/MyDrive/MIwDL/data/amr/abstract_meaning_representation_train.csv' if not load_no_duplicate_sets else 'drive/MyDrive/MIwDL/data/amr/train/amr_train_no_duplicate_inputs.csv')\n",
    "test_data_amr = pd.read_csv('drive/MyDrive/MIwDL/data/amr/abstract_meaning_representation_test.csv' if not load_no_duplicate_sets else 'drive/MyDrive/MIwDL/data/amr/test/amr_test_no_duplicate_inputs.csv')\n",
    "\n",
    "# Load the datasets for the Web NLG 2020 challenge\n",
    "#train_data_web_nlg = pd.read_csv('data/web_nlg/train/webNLG2020_train.csv' if not load_no_duplicate_sets else 'data/web_nlg/train/webNLG2020_train_no_duplicate_inputs.csv')\n",
    "#test_data_web_nlg = pd.read_csv('data/web_nlg/test/webNLG2020_test.csv' if not load_no_duplicate_sets else 'data/web_nlg/test/webNLG2020_test_no_duplicate_inputs.csv')\n",
    "\n",
    "# Load the datasets for the Meaning Representation E2E challenge\n",
    "#train_data_e2e = pd.read_csv('data/e2e/train/trainset.csv' if not load_no_duplicate_sets else 'data/e2e/train/trainset_no_duplicate_inputs.csv' )\n",
    "#test_data_e2e = pd.read_csv('data/e2e/test/testset_w_refs.csv' if not load_no_duplicate_sets else 'data/e2e/test/testset_no_duplicate_inputs.csv')\n",
    "\n",
    "# Load the datasets for the Abstract Meaning Representation AMR challenge\n",
    "#train_data_amr = pd.read_csv('data/amr/train/abstract_meaning_representation_train.csv' if not load_no_duplicate_sets else 'data/amr/train/amr_train_no_duplicate_inputs.csv')\n",
    "#test_data_amr = pd.read_csv('data/amr/test/abstract_meaning_representation_test.csv' if not load_no_duplicate_sets else 'data/amr/test/amr_test_no_duplicate_inputs.csv')False)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qRBcOeOFh0qU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# sort the values by ascending order\n",
    "test_data_web_nlg = test_data_web_nlg.sort_values(by='input_text', ignore_index=True)\n",
    "train_data_web_nlg = train_data_web_nlg.sort_values(by='input_text', ignore_index=True)\n",
    "\n",
    "test_data_e2e = test_data_e2e.sort_values(by='input_text', ignore_index=True)\n",
    "train_data_e2e = train_data_e2e.sort_values(by='input_text', ignore_index=True)\n",
    "\n",
    "train_data_amr = train_data_amr.sort_values(by='input_text', ignore_index=True)\n",
    "test_data_amr = test_data_amr.sort_values(by='input_text', ignore_index=True)\n",
    "\n",
    "# Trimming off the last few datapoints from Web NLG so hat a batch would not leave any remainder.\n",
    "train_data_web_nlg = train_data_web_nlg.iloc[:35200,:] if not load_no_duplicate_sets else train_data_web_nlg.iloc[:len(train_data_web_nlg)-3,:]\n",
    "test_data_web_nlg = test_data_web_nlg.iloc[:1720,:] if not load_no_duplicate_sets else test_data_web_nlg.iloc[:len(test_data_web_nlg)-7,:]\n",
    "\n",
    "# Trimming off the last few datapoints from E2E so that a batch would not leave any remainder.\n",
    "train_data_e2e = train_data_e2e.iloc[:len(train_data_e2e)-1,:] if not load_no_duplicate_sets else train_data_e2e.iloc[:len(train_data_e2e)-6,:]\n",
    "test_data_e2e = test_data_e2e.iloc[:len(test_data_e2e)-5,:] if not load_no_duplicate_sets else test_data_e2e.iloc[:len(test_data_e2e)-6,:]\n",
    "\n",
    "# Trimming off the last few datapoints from AMR so that a batch would not leave any remainder.\n",
    "train_data_amr = train_data_amr.iloc[:len(train_data_amr)-4,:] if not load_no_duplicate_sets else train_data_amr.iloc[:len(train_data_amr)-6,:]\n",
    "test_data_amr = test_data_amr.iloc[:len(test_data_amr)-6,:] if not load_no_duplicate_sets else test_data_amr.iloc[:len(test_data_amr)-3,:]"
   ],
   "metadata": {
    "id": "FrFGcPdzY6X2"
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Number of train batches Web NLG: 1640 --- \n",
      "--- Number of test  batches Web NLG: 74  --- \n",
      "\n",
      "--- Number of train batches E2E : 607 --- \n",
      "--- Number of test  batches E2E : 78  --- \n",
      "\n",
      "--- Number of train batches AMR : 168 --- \n",
      "--- Number of test  batches AMR : 18  --- \n"
     ]
    }
   ],
   "source": [
    "# Set the batch size and the number of training epochs\n",
    "batch_size_web_nlg = 8\n",
    "batch_size_e2e = 8\n",
    "batch_size_amr = 8\n",
    "\n",
    "number_of_batches_train_web_nlg = int(len(train_data_web_nlg)/batch_size_web_nlg)\n",
    "number_of_batches_test_web_nlg = int(len(test_data_web_nlg)/batch_size_web_nlg)\n",
    "\n",
    "number_of_batches_train_e2e = int(len(train_data_e2e)/batch_size_e2e)\n",
    "number_of_batches_test_e2e = int(len(test_data_e2e)/batch_size_e2e)\n",
    "\n",
    "number_of_batches_train_amr = int(len(train_data_amr)/batch_size_amr)\n",
    "number_of_batches_test_amr = int(len(test_data_amr)/batch_size_amr)\n",
    "\n",
    "print('--- Number of train batches Web NLG: ' + str(number_of_batches_train_web_nlg) + ' --- ')\n",
    "print('--- Number of test  batches Web NLG: ' + str(number_of_batches_test_web_nlg) + '  --- \\n')\n",
    "\n",
    "print('--- Number of train batches E2E : ' + str(number_of_batches_train_e2e) + ' --- ')\n",
    "print('--- Number of test  batches E2E : ' + str(number_of_batches_test_e2e) + '  --- \\n')\n",
    "\n",
    "print('--- Number of train batches AMR : ' + str(number_of_batches_train_amr) + ' --- ')\n",
    "print('--- Number of test  batches AMR : ' + str(number_of_batches_test_amr) + '  --- ')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RMhX5cjnh0qW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4ef33540-5277-441a-b324-ad99e34429d6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def create_list_of_batches(batch_size, num_batches, data, tokenizer):\n",
    "# Create List of batches for inputs and labels\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(num_batches):\n",
    "        input_batch=[]\n",
    "        label_batch=[]\n",
    "        for index,row in data[i*batch_size:i*batch_size+batch_size].iterrows():\n",
    "#          input_batch.append('translate from Graph to Text: '+row['input_text']+'</s>')\n",
    "#          label_batch.append(row['target_text']+'</s>')\n",
    "\n",
    "          input_batch.append('translate from Graph to Text: '+row['input_text'])\n",
    "          label_batch.append(row['target_text'])\n",
    "\n",
    "        input_batch=tokenizer.batch_encode_plus(input_batch,max_length=600,padding='max_length', return_tensors='pt')[\"input_ids\"]\n",
    "        label_batch=tokenizer.batch_encode_plus(label_batch,max_length=600,padding='max_length',return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "        input_batch=input_batch.to(dev)\n",
    "        label_batch=label_batch.to(dev)\n",
    "\n",
    "        inputs.append(input_batch)\n",
    "        labels.append(label_batch)\n",
    "    return inputs, labels\n",
    "\n",
    "\"\"\"inputs_train_web_nlg, \\\n",
    "labels_train_web_nlg = create_list_of_batches(batch_size=batch_size_web_nlg,\n",
    "                                              num_batches=number_of_batches_train_web_nlg,\n",
    "                                              data=train_data_web_nlg,\n",
    "                                              tokenizer=tokenizer_t5_small)\n",
    "inputs_test_web_nlg, \\\n",
    "labels_test_web_nlg = create_list_of_batches(batch_size=batch_size_web_nlg,\n",
    "                                              num_batches=number_of_batches_test_web_nlg,\n",
    "                                              data=test_data_web_nlg,\n",
    "                                              tokenizer=tokenizer_t5_small)\n",
    "inputs_train_e2e, \\\n",
    "labels_train_e2e = create_list_of_batches(batch_size=batch_size_e2e,\n",
    "                                              num_batches=number_of_batches_train_e2e,\n",
    "                                              data=train_data_e2e,\n",
    "                                              tokenizer=tokenizer_t5_small)\n",
    "inputs_test_e2e, \\\n",
    "labels_test_e2e = create_list_of_batches(batch_size=batch_size_e2e,\n",
    "                                              num_batches=number_of_batches_test_e2e,\n",
    "                                              data=test_data_e2e,\n",
    "                                              tokenizer=tokenizer_t5_small)\"\"\"\n",
    "inputs_train_amr, \\\n",
    "labels_train_amr = create_list_of_batches(batch_size=batch_size_amr,\n",
    "                                              num_batches=number_of_batches_train_amr,\n",
    "                                              data=train_data_amr,\n",
    "                                              tokenizer=tokenizer_t5_small)\n",
    "inputs_test_amr, \\\n",
    "labels_test_amr = create_list_of_batches(batch_size=batch_size_amr,\n",
    "                                              num_batches=number_of_batches_test_amr,\n",
    "                                              data=test_data_amr,\n",
    "                                              tokenizer=tokenizer_t5_small)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sxy2hYEYh0qX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Set the Optimizer with Parameter values suggested for T5"
   ],
   "metadata": {
    "collapsed": false,
    "id": "j4u8a5oqh0qY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def optimizer_adafactor(model, \n",
    "              lr=1e-3,              # default values for adafactor\n",
    "              eps=(1e-30, 1e-3),    # default values for adafactor\n",
    "              clip_threshold=1.0,   # default values for adafactor \n",
    "              decay_rate=-0.8,      # default values for adafactor\n",
    "              beta1=None,           # default values for adafactor \n",
    "              weight_decay=0.0,     # default values for adafactor \n",
    "              relative_step=False,   \n",
    "              scale_parameter=False,\n",
    "              warmup_init=False):\n",
    "  \n",
    "    return Adafactor(\n",
    "        [p for n, p in model.named_parameters() if n == \"soft_prompt.weight\"],\n",
    "        lr=lr,\n",
    "        eps=eps,\n",
    "        clip_threshold=clip_threshold,\n",
    "        decay_rate=decay_rate,\n",
    "        beta1=beta1,\n",
    "        weight_decay=weight_decay,\n",
    "        relative_step=relative_step,\n",
    "        scale_parameter=scale_parameter,\n",
    "        warmup_init=warmup_init\n",
    "    )\n",
    "optimizer_t5 = optimizer_adafactor(model_t5_small)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VKXdRP2yh0qY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Routine Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "uNBoD0Mth0qZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def progress(loss,value, max=100):\n",
    "    return HTML(\"\"\" Batch loss :{loss}\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(loss=loss,value=value, max=max))\n",
    "\n",
    "def trainer(model, num_batches, inputs, labels, optimizer, challenge_name, model_name):\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    loss_per_10_steps=[]\n",
    "    for epoch in range(1,epochs+1):\n",
    "      print('Running epoch: {}'.format(epoch))\n",
    "      running_loss=0\n",
    "\n",
    "      out = display(progress(1, num_batches+1), display_id=True)\n",
    "      for i in range(num_batches):\n",
    "\n",
    "        # clear out the gradients of all Variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propogation\n",
    "        outputs = model(input_ids=inputs[i], labels=labels[i])\n",
    "        loss = outputs.loss\n",
    "        loss_num=loss.item()\n",
    "        logits = outputs.logits\n",
    "        running_loss+=loss_num\n",
    "        if i%10 == 0: loss_per_10_steps.append(loss_num)\n",
    "        out.update(progress(loss_num,i, num_batches+1))\n",
    "\n",
    "        # calculating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #updating the params\n",
    "        optimizer.step()\n",
    "\n",
    "      running_loss=running_loss/int(num_batches)\n",
    "      print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
    "\n",
    "      # plot the loss\n",
    "      steps = [i for i in range(len(loss_per_10_steps))]\n",
    "      plt.plot(steps, loss_per_10_steps)\n",
    "      plt.title(f'Loss curve for the {challenge_name} challenge trained for {epochs} epochs on T5-{model_name}')\n",
    "      plt.xlabel('Steps')\n",
    "      plt.ylabel('Loss')\n",
    "      plt.show()\n",
    "    return model\n",
    "\n",
    "epochs = 1\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "osZOfoXAh0qZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Process"
   ],
   "metadata": {
    "id": "mQG9jyovcPZc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train T5 small on a mixture of all three challenges\n",
    "\n",
    "model_t5_small = trainer(model=model_t5_small, \n",
    "                         num_batches=number_of_batches_train_amr, \n",
    "                         inputs=inputs_train_amr, \n",
    "                         labels=labels_train_amr,\n",
    "                         optimizer=optimizer_t5,\n",
    "                         challenge_name='AMR',\n",
    "                         model_name='small')"
   ],
   "metadata": {
    "id": "DZDXEP8bW21T",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "outputId": "d7585043-ed9f-48bd-b0cb-350aa2ab689d"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       " Batch loss :1.902909755706787\n",
       "        <progress\n",
       "            value='167'\n",
       "            max='169',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            167\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 , Running loss: 2.094692670163654\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Flush memory after interrupting training\n",
    "#@markdown This will *hopefully* prevent a CUDA out-of-memory error.\n",
    "try:\n",
    "  del input_ids\n",
    "except Exception:\n",
    "  pass\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ],
   "metadata": {
    "id": "jEHrothmDVVC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate a Model"
   ],
   "metadata": {
    "id": "1lcpAnNzxIKC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# use this method to generate text for each test input ids, then save the predictions\n",
    "# in a file 'hypothesis' for web nlg and amr to later use the official evaluation script for the \n",
    "# challenges\n",
    "def make_predictions(model, inputs_test, tokenizer, challenge_name):\n",
    "\n",
    "  model_predictions = []\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i in range(len(inputs_test)):\n",
    "      print(i)\n",
    "      output = tokenizer.batch_decode(model.generate(inputs_test[i],\n",
    "                                                     do_sample=True, \n",
    "                                                     max_length=400,\n",
    "                                                     top_p=0.92,\n",
    "                                                     top_k=0),\n",
    "                                      skip_special_tokens=True)\n",
    "      model_predictions.append([x.replace('<pad>','').replace('</s>','').strip() for x in output])\n",
    "\n",
    "    # flatten the predictions list which has the length of batch_size * number_of_batches\n",
    "    model_predictions = list(chain(*model_predictions))  \n",
    "  model.train()\n",
    "  with open('drive/MyDrive/MIwDL/data/' + challenge_name + '/test/prompt_tuning_hypothesis/hypothesis', 'w') as file:\n",
    "    for i in range(len(model_predictions)):\n",
    "      file.write(model_predictions[i] + '\\n' if i < len(model_predictions)-1 else model_predictions[i])\n",
    "  return model_predictions"
   ],
   "metadata": {
    "id": "vHYrhlgB_ZDe"
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_predictions = make_predictions(model=model_t5_small,\n",
    "                         inputs_test=inputs_test_amr,\n",
    "                         tokenizer=tokenizer_t5_small,\n",
    "                         challenge_name='amr')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "UkCgGtDRsP0P",
    "outputId": "bc1e3ac6-4df0-4683-cd06-5a82e424b7cc"
   },
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-66-3dbbaecc18d7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m                          \u001B[0minputs_test\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs_test_amr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                          \u001B[0mtokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenizer_t5_small\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m                          challenge_name='amr')\n\u001B[0m",
      "\u001B[0;32m<ipython-input-65-7cc48df34672>\u001B[0m in \u001B[0;36mmake_predictions\u001B[0;34m(model, inputs_test, tokenizer, challenge_name)\u001B[0m\n\u001B[1;32m     13\u001B[0m                                                      \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m400\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m                                                      \u001B[0mtop_p\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.92\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m                                                      top_k=0),\n\u001B[0m\u001B[1;32m     16\u001B[0m                                       skip_special_tokens=True)\n\u001B[1;32m     17\u001B[0m       \u001B[0mmodel_predictions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'<pad>'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'</s>'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1144\u001B[0m                 \u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict_in_generate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1145\u001B[0m                 \u001B[0msynced_gpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msynced_gpus\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1146\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1147\u001B[0m             )\n\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36msample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1648\u001B[0m                 \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1649\u001B[0m                 \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1650\u001B[0;31m                 \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1651\u001B[0m             )\n\u001B[1;32m   1652\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'decoder_input_ids'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the Model"
   ],
   "metadata": {
    "id": "oIFaZMLtsiU6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model_t5_small_web_nlg.state_dict(),f'drive/MyDrive/MIwDL/models/25_epochs_t5_small/t5_small_web_nlg_{epochs}_epochs_batch_size_{batch_size_web_nlg}_optimized_inputs.bin')"
   ],
   "metadata": {
    "id": "710KzDLLsqxW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model_t5_small_e2e.state_dict(),f'drive/MyDrive/MIwDL/models/25_epochs_t5_small/t5_small_e2e_{epochs}_epochs_batch_size_{batch_size_e2e}.bin')"
   ],
   "metadata": {
    "id": "URldTyRmuCYk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model_t5_base_amr.state_dict(),f'drive/MyDrive/MIwDL/models/t5_base/t5_base_amr_{epochs}_epochs_batch_size_{batch_size_amr}.bin')"
   ],
   "metadata": {
    "id": "hzI_fWoRW0s_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load a model"
   ],
   "metadata": {
    "id": "7JhDhOJ2r5gX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json\n",
    "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n",
    "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-config.json\n",
    "\n",
    "model_t5_small_e2e = T5ForConditionalGeneration.from_pretrained('drive/MyDrive/MIwDL/models/25_epochs_t5_small/t5_small_web_nlg_25_epochs_batch_size_8.bin', \n",
    "                                                                    return_dict=True,\n",
    "                                                                    config='t5-small-config.json')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "model_t5_small_e2e.to(dev)\n",
    "\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained('t5-small')\n"
   ],
   "metadata": {
    "id": "jbYi2PuIr5De"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "prompt_tuning_t5.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "OIoYkDMph0qI",
    "qtPNBfoDh0qQ",
    "YA92MUkIh0qU",
    "j4u8a5oqh0qY",
    "uNBoD0Mth0qZ",
    "mQG9jyovcPZc",
    "oIFaZMLtsiU6",
    "7JhDhOJ2r5gX"
   ],
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}